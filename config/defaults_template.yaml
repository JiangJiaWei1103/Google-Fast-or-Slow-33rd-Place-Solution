# Common configuration for training and evaluation processes
# ==Training==
seed: 168
device: 'cuda:0'
epochs: 1
loss_fn:
    # Name of the loss criterion, the choices are as follows:
    #    {'l1', 'l2', 'mtk'}
    name: 'l1'
    # Multitask coordination
    mtk:
        # Supervised task
        #supr: 'l1'
        # Regularization
        #ae_recons: 0
        #vq: 0
    rescale: True
# Model checkpoint
model_ckpt:
    ckpt_metric: null
    ckpt_mode: null
    best_ckpt_mid: last

# ==DataLoader==
dataloader:
    batch_size: 128
    shuffle: True
    num_workers: 4

# ==Solver==
solver:
    # ==Optimizer==
    optimizer:
        name: adam
        # Learning rate
        lr: 0.001
        # Weight decay, default=1e-2 for AdamW
        weight_decay: 0.0001
        # ==Ada* series==
        # Term added to the denominator to improve numerical stability
        eps: 1e-8
        # ==Adadelta==
        # Coefficient used for computing a running avg of squared gradients
        rho: null
        # ==Adagrad==
        # Learning rate decay
        lr_decay: null
        # ==Adam* series==
        # Coefficients used for computing running avg of grad and its square
        beta1: 0.9
        beta2: 0.999
        #  Whether to use the AMSGrad variant
        amsgrad: False
        # ==SGD==
        # Momentum factor
        momentum: null
        # Dampening for momentum
        dampening: null
        # Enable Nesterov momentum
        nesterov: null

    # ==Learning rate scheduler==
    lr_skd:
        name: plateau
        milestones: null
        # Multiplicative factor of learning rate decay
        gamma: null

        T_max: null
        eta_min: null

        mode: min
        factor: 0.5
        patience: 5

# ==Early Stopping==
es:
    patience: 10
    mode: 'min'

# ==Evaluator==
evaluator:
    # Evaluation metrics, the choices are as follows:
    #     {'mae', 'rmse', 'rae', 'rrse', 'corr', 'mmae', 'mrmse', 'mmape'}
    eval_metrics: ["one_minus_slowdown", "ndcg"]
